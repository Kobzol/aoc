1) numactl
Pinning - taskset -c 0,1 ./program

numactl -H - hardware description
numactl -l - allocate on local node only

- can be specified manually with membind
MPOL_BIND - specific nodes
MPOL_INTERLEAVE - interleaved allocation
MPOL_PREFERRED - preferably local
MPOL_LOCAL - only local (but still goes remote when low on memory)

Linux first-touch policy - memory is physically allocated on first touch, not on allocation

numactl --cpubind=0 --cpunodebind=0     - allocate on  node 0 only
numactl --membind=0,1                   - allocate memory from nodes 0 and 1
numactl --interleave=0,1,2              - interleave with round robin between 0, 1 and 2
numactl --physcpubind=0,1               - only run on CPUs 0 and 1

numactl can also control policies for tmpfs, huge page FS and shared memory files

2) stream.c

3) Singlethread STREAMS test
./stream                                    - 13 Gb/s (enough memory, same as local), 19 Gb/s ICC Copy
numactl -l ./stream                         - 13 Gb/s
numactl --cpubind=0 --membind=0 ./stream    - 13 Gb/s
numactl --cpubind=0 --membind=1 ./stream    - 10 Gb/s
numactl --interleave=all ./stream           - 11.9 Gb/s

4) Multithread STREAMS test
OMP_NUM_THREADS=12 numactl --cpubind=0 --membind=0 ./stream         - memory bottleneck, 44 Gb/s
OMP_NUM_THREADS=12 numactl --cpubind=0 --membind=1 ./stream         - remote access, 17.5 Gb/s
OMP_NUM_THREADS=12 numactl --cpubind=0 --interleave=all ./stream    - 33 Gb/s
OMP_NUM_THREADS=12 numactl --interleave=all ./stream                - 54.5 Gb/s

OMP_NUM_THREADS=24 numactl ./stream                                 - 88 Gb/s
OMP_NUM_THREADS=24 numactl --interleave=all ./stream                - 59 Gb/s

ICC 24 threads copy - 103 Gb/s

Stuff:
icc, gcc compile flags

numastat - NUMA node allocation statistics
numastat diff - 600k, matches alloc size

get NUMA node of page - move_pages
get NUMA node of CPU - numa_node_of_cpu(sched_getcpu())

allocated memory - /proc/meminfo, anonpages - mmap used for big mallocs, perf
cache size - getconf -a | grep CACHE
page size - getpagesize()

#pragma omp parallel for - schedule
OMP_PROC_BIND, OMP_PLACES

fourth faster than third - more L3 cache misses, reverse array direction
- non-temporal access

FMA:
a[j] = b[j]+scalar*c[j];
401ef6:	c4 c2 f1 a9 04 c3    	vfmadd213sd (%r11,%rax,8),%xmm1,%xmm0
